base_model: /shared/base_models/Qwen2.5-14B
# model_type: AutoModelForCausalLM
# tokenizer_type: AutoTokenizer
is_qwen_derived_model: true
is_llama_derived_model: false
resume_from_checkpoint:
#auto_resume_from_checkpoints: false
seed: 42
load_in_8bit: false
load_in_4bit: false
strict: false
shuffle_merged_datasets: true
trust_remote_code:
bf16: auto
fp16:
tf32: false
special_tokens:
  # bos_token: "<|im_start|>"
  # eos_token: "<|im_end|>"
  # pad_token: "<|endoftext|>"
  additional_special_tokens:
    - "<|im_header_end|>"
    - "<functions>"
    - "</functions>"
    - "<function_calls>"
    - "</function_calls>"
    - "</function>"
    - "<think>"
    - "</think>"
    - "<function name=\""
    - "<function_outputs>"
    - "<output name=\""
    - "<output>"
    - "</function_outputs>"

model_init_kwargs:
  init_device: "meta"
  resize_token_embeddings_strategy: "mean"
# bfloat16: true # require >=ampere
# float16: true
chat_template: qwen_25
datasets:
  - path: json
    data_files: /shared/final_1_5M.jsonl  # replace this with the path to your dataset
    type: chat_template
    field_messages: messages
trust_remote_code: true
dataset_prepared_path: /shared/data/custom_v1/dataset/14b # replace this with the path to your prepared dataset
#dataset_processes: 200
hub_model_id:
sequence_len: 8192
pad_to_sequence_len: true
sample_packing: false
#batch_flattening:

wandb_project: project-r1-train-v1-14b # replace this with your wandb project name
wandb_entity: ankit-singh-5techlab # replace this with your wandb entity name
wandb_watch:
wandb_name: train-r1-train-v1-14b # replace this with your wandb run name
wandb_run_id: run-r1-train-v1-14b # replace this with your wandb run id
wandb_log_model:
output_dir: /shared/data/train/model/14b/v1 # replace this with the path to your output directory
###########
gradient_accumulation_steps: 8 # change this to your desired gradient accumulation steps
micro_batch_size: 1 # change this to your desired micro batch size
num_epochs: 3 # change this to your desired number of epochs
warmup_ratio: 0.1 # change this to your desired warmup ratio
learning_rate: 1e-5 # change this to your desired learning rate

eval_steps: 100
save_steps: 100
save_total_limit: 10
#auto_find_batch_size:
load_best_model_at_end: true
metric_for_best_model: "eval_loss"
greater_is_better: false
####
eval_table_size: 1
train_on_inputs: false


gradient_checkpointing: true
# gradient_checkpointing_kwargs:
#   use_reentrant: true
early_stopping_patience: 3
lr_scheduler: cosine

weight_decay: 0.05 # change this to your desired weight decay
max_grad_norm: 1.0 # change this to your desired max grad norm
# dropout: 0.01
xformers_attention:
deepspeed: /shared/data/axolotl/deepspeed_configs/zero3_bf16_cpuoffload_params.json # replace this with the path to your deepspeed config file

flash_attention: true
optimization:
 use_liger: true
 flash_attention_implementation: "variant-3-liger"

optimizer: paged_adamw_8bit
#
#tokenizer_config:
#  padding_side: left
#

plugins:
  - axolotl.integrations.liger.LigerPlugin
liger_rope: true
liger_rms_norm: true
liger_glu_activation: true
liger_layer_norm: true
liger_fused_linear_cross_entropy: true


val_set_size: 0.01
do_eval: true

save_on_each_node: false
