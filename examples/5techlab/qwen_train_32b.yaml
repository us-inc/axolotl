base_model: /shared/base_models/qwen2.5-32b/
# model_type: AutoModelForCausalLM
# tokenizer_type: AutoTokenizer
is_qwen_derived_model: true
is_llama_derived_model: false
resume_from_checkpoint:
#auto_resume_from_checkpoints: false
seed: 42
load_in_8bit: false
load_in_4bit: false
strict: false
shuffle_merged_datasets: true
trust_remote_code:
bf16: auto
fp16:
tf32: false
special_tokens:
  # bos_token: "<|im_start|>"
  # eos_token: "<|im_end|>"
  # pad_token: "<|endoftext|>"
  additional_special_tokens:
    - "<|im_header_end|>"
    - "<functions>"
    - "</functions>"
    - "<function_calls>"
    - "</function_calls>"
    - "</function>"
    - "<think>"
    - "</think>"
    - "<function name=\""
    - "<function_outputs>"
    - "<output name=\""
    - "<output>"
    - "</function_outputs>"

model_init_kwargs:
  init_device: "meta"
  resize_token_embeddings_strategy: "mean"
# bfloat16: true # require >=ampere
# float16: true
chat_template: qwen_25
datasets:
  - path: json
    data_files: /shared/dataset_chat_2_4M.jsonl
    type: chat_template
    field_messages: messages
trust_remote_code: true
dataset_prepared_path: /shared/data/2_4M/last_run_prepared
# dataset_processes: 200
hub_model_id:
sequence_len: 8192
pad_to_sequence_len: true
sample_packing: false
#batch_flattening:

wandb_project: train-r1-2_4M
wandb_entity: ankit-singh-5techlab
wandb_watch:
wandb_name: train-v1_32B-2_4M
wandb_run_id: run-2_4M_tmp
wandb_log_model:
output_dir: /shared/data/2_4M/32b
###########
gradient_accumulation_steps: 6
micro_batch_size: 1
num_epochs: 3
warmup_ratio: 0.1
learning_rate: 1e-5

eval_steps: 100
save_steps: 100
save_total_limit:
#auto_find_batch_size:
load_best_model_at_end: true
metric_for_best_model: "eval_loss"
greater_is_better: false
####
eval_table_size: 1
train_on_inputs: false


gradient_checkpointing: true
# gradient_checkpointing_kwargs:
#   use_reentrant: true
early_stopping_patience: 3
lr_scheduler: cosine

weight_decay: 0.01
max_grad_norm: 1.0
# dropout: 0.01
xformers_attention:
deepspeed: deepspeed_configs/zero3_bf16.json

flash_attention: true
optimization:
 use_liger: true
 flash_attention_implementation: "variant-3-liger"

optimizer: adamw_torch_fused
#
#tokenizer_config:
#  padding_side: left
#

plugins:
  - axolotl.integrations.liger.LigerPlugin
liger_rope: true
liger_rms_norm: true
liger_glu_activation: true
liger_layer_norm: true
liger_fused_linear_cross_entropy: true


val_set_size: 0.01
do_eval: true

save_on_each_node: false
